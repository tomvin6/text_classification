{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "in order to install NLTK stop words, run the following python commands -\n",
    "\n",
    "import nltk\n",
    "nltk.download()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Predict team using defect title"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "in the following notebook, a model is built and trained using defects data from center. \n",
    "for predicting one field from defects form - team field."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Required libraries\n",
    "general imports - keras, sklearn, numpy, pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "import keras \n",
    "from keras.models import Sequential\n",
    "from keras.layers import Activation\n",
    "from keras.optimizers import Adam\n",
    "from keras import regularizers\n",
    "from keras.layers import Dense, GlobalAveragePooling1D, Embedding, Dropout, Conv1D, MaxPooling1D, Flatten\n",
    "from keras.callbacks import EarlyStopping\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from sklearn import model_selection\n",
    "from sklearn import metrics\n",
    "from keras.metrics import categorical_crossentropy\n",
    "from sklearn.model_selection import train_test_split\n",
    "import pandas as pd\n",
    "import json\n",
    "import numpy as np\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# load data from dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are: 28970 records of defects in dataset.\n"
     ]
    }
   ],
   "source": [
    "# Reads and converts json / Excel format to python dict.\n",
    "def js_r(data):\n",
    "   with open(data, encoding='utf-8') as f_in:\n",
    "       return(json.load(f_in))\n",
    "\n",
    "def read_excel(path):\n",
    "    return pd.read_excel(open(path, 'rb'),\n",
    "                  sheet_name='Sheet2')  # doctest: +SKIP\n",
    "\n",
    "# load JSON data\n",
    "# data_path=r'C:\\dev\\defects.json'\n",
    "# my_dic_data = js_r(data_path)['data']\n",
    "\n",
    "# Load EXCEL data:\n",
    "data_path = r'datasets' + os.sep + 'defects-all-spread.xlsx'\n",
    "my_dic_data = read_excel(data_path)\n",
    "print('There are: ' + str(len(my_dic_data)) + ' records of defects in dataset.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Transform & normalize dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "handling ids fields from json dataset, remove unused columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "def transform_data(data_list):\n",
    "    print(len(data_list))\n",
    "    for defect in my_dic_data:\n",
    "        for column in defect.keys():\n",
    "            if not defect[column] == None and type(defect[column]) is dict and 'id' in defect[column]:\n",
    "                defect[column] = defect[column]['id']\n",
    "        \n",
    "    return pd.DataFrame(my_dic_data).drop(['access_granted', 'blocked_reason', 'path', 'phase',\n",
    "                                           'blocked', 'original_id', 'priority', 'defect_root_level', 'story_points', 'user_tags', 'program', 'taxonomies', 'version_stamp', 'detected_in_build', 'sprint', 'dependency_problem_type', 'defect_type'], axis=1)\n",
    "# df = transform_data(my_dic_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "cleaning the dataset (removing defects without labels, defects with only one label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "An example of dataset structure:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>creation_time</th>\n",
       "      <th>id</th>\n",
       "      <th>name</th>\n",
       "      <th>description</th>\n",
       "      <th>knowledge_modified_udf</th>\n",
       "      <th>parent.name</th>\n",
       "      <th>parent.id</th>\n",
       "      <th>release.id</th>\n",
       "      <th>team.id</th>\n",
       "      <th>team.name</th>\n",
       "      <th>product_areas.id</th>\n",
       "      <th>product_areas.name</th>\n",
       "      <th>application_modules.id</th>\n",
       "      <th>application_modules.name</th>\n",
       "      <th>qa_owner.id</th>\n",
       "      <th>qa_owner.name</th>\n",
       "      <th>owner.id</th>\n",
       "      <th>owner.name</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2017-01-11 09:05:12</td>\n",
       "      <td>205022</td>\n",
       "      <td>[Regression Day - Firefox] - Exception thrown ...</td>\n",
       "      <td>&amp;lt;html&amp;gt;&amp;lt;body&amp;gt;&amp;lt;p&amp;gt;1. create tes...</td>\n",
       "      <td>NaT</td>\n",
       "      <td>Requirement Backlog</td>\n",
       "      <td>1001</td>\n",
       "      <td>22001.0</td>\n",
       "      <td>3002.0</td>\n",
       "      <td>OMG Yuval (Do Not Use)</td>\n",
       "      <td>78018.0</td>\n",
       "      <td>10 Tests</td>\n",
       "      <td>78018.0</td>\n",
       "      <td>10 Tests</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>9001.0</td>\n",
       "      <td>sari.bivas@microfocus.com</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2016-12-27 10:06:39</td>\n",
       "      <td>198001</td>\n",
       "      <td>Bug Hunt 12.53.19 - when add \"On it\" user I ge...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaT</td>\n",
       "      <td>Requirement Backlog</td>\n",
       "      <td>1001</td>\n",
       "      <td>22001.0</td>\n",
       "      <td>8003.0</td>\n",
       "      <td>Sharon - XMen Magneto</td>\n",
       "      <td>79021.0</td>\n",
       "      <td>Failure Analysis</td>\n",
       "      <td>79021.0</td>\n",
       "      <td>Failure Analysis</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>3035.0</td>\n",
       "      <td>abed.masrawa@hpe.com</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        creation_time      id  \\\n",
       "0 2017-01-11 09:05:12  205022   \n",
       "1 2016-12-27 10:06:39  198001   \n",
       "\n",
       "                                                name  \\\n",
       "0  [Regression Day - Firefox] - Exception thrown ...   \n",
       "1  Bug Hunt 12.53.19 - when add \"On it\" user I ge...   \n",
       "\n",
       "                                         description knowledge_modified_udf  \\\n",
       "0  &lt;html&gt;&lt;body&gt;&lt;p&gt;1. create tes...                    NaT   \n",
       "1                                                NaN                    NaT   \n",
       "\n",
       "           parent.name  parent.id  release.id  team.id  \\\n",
       "0  Requirement Backlog       1001     22001.0   3002.0   \n",
       "1  Requirement Backlog       1001     22001.0   8003.0   \n",
       "\n",
       "                team.name  product_areas.id product_areas.name  \\\n",
       "0  OMG Yuval (Do Not Use)           78018.0           10 Tests   \n",
       "1   Sharon - XMen Magneto           79021.0   Failure Analysis   \n",
       "\n",
       "   application_modules.id application_modules.name  qa_owner.id qa_owner.name  \\\n",
       "0                 78018.0                 10 Tests          NaN           NaN   \n",
       "1                 79021.0         Failure Analysis          NaN           NaN   \n",
       "\n",
       "   owner.id                 owner.name  \n",
       "0    9001.0  sari.bivas@microfocus.com  \n",
       "1    3035.0       abed.masrawa@hpe.com  "
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# build dataframe from dict\n",
    "df = pd.DataFrame(my_dic_data)\n",
    "# cleaning rows without labels OR descritpion\n",
    "df = df[df['team.id'].map(lambda x: x != None)]\n",
    "df = df[df['name'].map(lambda x: x != None)]\n",
    "train_df = df[df['description'].map(lambda x: x != None)]\n",
    "train_df = train_df.groupby('team.id').filter(lambda x : len(x)>2)\n",
    "\n",
    "# building translation map from team id -> team name\n",
    "team_id_to_name_map = pd.Series(df['team.name'].values,index=df['team.id']).to_dict()\n",
    "# spliting Lables from features\n",
    "X = train_df.drop(['team.id'], axis=1)\n",
    "Y = train_df['team.id']\n",
    "\n",
    "print(\"An example of dataset structure:\")\n",
    "train_df.head(n=2)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "labels encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "there are: 34 different labels in dataset, which corresponds to all teams\n"
     ]
    }
   ],
   "source": [
    "# label envoding\n",
    "from sklearn import preprocessing\n",
    "le = preprocessing.LabelEncoder()\n",
    "encoded_labels = pd.DataFrame(le.fit_transform(train_df['team.id']))\n",
    "classes_number = encoded_labels.groupby(0).nunique().shape[0]\n",
    "all_labels = encoded_labels[0].unique()\n",
    "print('there are: ' + str(classes_number) + ' different labels in dataset, which corresponds to all teams')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train samples: (22576, 17) validation samples: (5644, 17)\n"
     ]
    }
   ],
   "source": [
    "# ensure that the model is not overfitting \n",
    "# train & validation tests with labels, test without\n",
    "xtrain, xvalid, ytrain, yvalid = train_test_split(X, encoded_labels,\n",
    "                                                      stratify=encoded_labels,\n",
    "                                                      random_state=42,\n",
    "                                                      test_size=0.2, shuffle=True)\n",
    "\n",
    "\n",
    "print ('train samples: ' + str(xtrain.shape) + ' validation samples: ' + str(xvalid.shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Build Model + Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import metrics\n",
    "# parameters:BATCH_SIZE=16, EPOC_SIZE=30\n",
    "BATCH_SIZE = 50 # smaller batch size consume less memory (but can decrease accuracy)\n",
    "EPOC_SIZE = 30\n",
    "tokenizer_file_name = \"models\" + os.sep + \"FTtokenizer_\" + str(BATCH_SIZE) + \"B_\" + str(EPOC_SIZE) + \"E\" + \".pkl\"\n",
    "classifier_file_name = \"models\" + os.sep + \"FTClassifier_\" + str(BATCH_SIZE) + \"B_\" + str(EPOC_SIZE) + \"E\" + \".pkl\"\n",
    "\n",
    "class fasttext_classifier(object):\n",
    "    def __init__(self):\n",
    "        self.train_df = None\n",
    "        self.train_X = None\n",
    "        self.train_Y = None\n",
    "        self.vslid_X = None\n",
    "        self.valid_Y = None\n",
    "        self.model = None\n",
    "        self.hist = None\n",
    "        self.tokenizer = None\n",
    "        self.rare_train_words = []\n",
    "    \n",
    "    def create_model(self, input_dim, classes_number, embedding_dims=32, optimizer='adam'):\n",
    "        self.labels = all_labels\n",
    "        self.model = Sequential()\n",
    "        \n",
    "        # we start off with an efficient embedding layer which maps\n",
    "        # our vocab indices into embedding_dims dimensions\n",
    "        self.model.add(Embedding(input_dim=input_dim, output_dim=embedding_dims, input_length=256))\n",
    "        # Dropout consists in randomly setting a fraction rate of input units to 0 at each update during training time,\n",
    "        # which helps prevent overfitting.\n",
    "        self.model.add(Dropout(0.3))\n",
    "        #This layer creates a convolution kernel that is convolved with the layer input over a single spatial (or temporal) dimension to produce a tensor of outputs. If use_bias is True, a bias vector is created and added to the outputs. Finally, if activation is not None, it is applied to the outputs as well.\n",
    "        self.model.add(Conv1D(64,\n",
    "                              5,\n",
    "                              padding='valid',\n",
    "                              activation='relu'))\n",
    "        self.model.add(Dropout(0.3))\n",
    "        self.model.add(MaxPooling1D())\n",
    "        self.model.add(Flatten())\n",
    "        self.model.add(Dense(800, activation='relu'))\n",
    "        self.model.add(Dropout(0.5))\n",
    "        self.model.add(Dense(classes_number, activation='softmax'))\n",
    "\n",
    "        self.model.compile(loss='sparse_categorical_crossentropy',\n",
    "                           optimizer=optimizer,\n",
    "                           metrics=['accuracy'])\n",
    "\n",
    "        return\n",
    "\n",
    "    def set_tokenizer(self, tokenizer):\n",
    "        self.tokenizer = tokenizer\n",
    "\n",
    "    def train(self, docstrain, ytrain, docsvalid, yvalid):\n",
    "        self.train_X = docstrain\n",
    "        self.train_Y = ytrain\n",
    "        self.hist = self.model.fit(docstrain, ytrain,\n",
    "                                   batch_size=BATCH_SIZE,\n",
    "                                   validation_data=(docsvalid, yvalid),\n",
    "                                   epochs=EPOC_SIZE, verbose=2,\n",
    "                                   callbacks=[EarlyStopping(patience=4, monitor='val_loss')])\n",
    "\n",
    "        predictions = self.model.predict_proba(docsvalid)\n",
    "        predictions_classes = self.model.predict_classes(docsvalid)\n",
    "        try:\n",
    "            # print(\"valid shape=\" + str(yvalid.shape) + \"predictions shape=\" + str(predictions.shape) + \"labels shape=\" + str(self.labels.shape))\n",
    "            print(\"accuracy on validation set after training: %0.3f\" % (\n",
    "                np.sum(predictions_classes == yvalid[0]) / len(yvalid[0])))\n",
    "#             print(\"prediction rows: \" + str(len(predictions.values)))\n",
    "#             print(\"prediction columns: \" + str(len(predictions.values[0])))\n",
    "#             print(\"fasttext logloss: %0.3f \" % metrics.log_loss(yvalid[0].values, predictions.values, labels=self.labels[0].values))\n",
    "        except Exception as e:\n",
    "            print(\"Oops!! occured. could not calculate metrics for this epoc\")\n",
    "            print(e)\n",
    "        return\n",
    "\n",
    "    def predict(self, docstest):\n",
    "        print(\"run prediction\")\n",
    "        predictions = self.model.predict_proba(docstest)\n",
    "        predictions_classes = self.model.predict_classes(docstest)\n",
    "        return predictions, predictions_classes\n",
    "\n",
    "    def plot_train_vs_val(self):\n",
    "        hist = self.model.history\n",
    "        hist_dict = hist.history\n",
    "        # plot loss\n",
    "        fig = plt.figure()\n",
    "        plt.subplot(211)\n",
    "        val_loss = hist_dict.get('val_loss')\n",
    "        val_loss_line = plt.plot(val_loss, label='val_loss')\n",
    "        plt.legend()\n",
    "        loss = hist_dict.get('loss')\n",
    "        plt.plot(loss, label='train_loss')\n",
    "        plt.legend()\n",
    "        plt.title(\"train and validation loss\")\n",
    "        plt.ylabel(\"loss\")\n",
    "\n",
    "        # plot accuracy\n",
    "        plt.subplot(212)\n",
    "        val_acc = hist_dict.get('val_acc')\n",
    "        plt.plot(val_acc, label='val_acc')\n",
    "        plt.legend()\n",
    "        acc = hist_dict.get('acc')\n",
    "        plt.plot(acc, label='train_acc')\n",
    "        plt.legend()\n",
    "        plt.title(\"train and validation accuracy\")\n",
    "        plt.ylabel(\"accuracy\")\n",
    "        plt.xlabel(\"step\")\n",
    "\n",
    "        fig.savefig(\"fast-text-itr-performance.pdf\", format='pdf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.layers import Dense, GlobalAveragePooling1D, Embedding\n",
    "from keras.models import Sequential\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from sklearn import model_selection\n",
    "from sklearn import metrics\n",
    "from nltk.corpus import stopwords\n",
    "import pickle\n",
    "\n",
    "# preproceeings are:\n",
    "# Separate punctuation from words\n",
    "# Remove lower frequency words ( <= 2)\n",
    "# Cut a longer document which contains 256 words\n",
    "def preprocess(text, stop_words):\n",
    "    text = text.replace(\"' \", \" ' \")\n",
    "    signs = set(',.:;\"?![]')\n",
    "    prods = set(text) & signs\n",
    "    \n",
    "    if not prods:\n",
    "        return text\n",
    "\n",
    "    for sign in prods:\n",
    "        text = text.replace(sign, ' {} '.format(sign))\n",
    "    \n",
    "    if stop_words is not None:\n",
    "        # Remove Stopwords\n",
    "        text = ' '.join([w for w in text.split(' ') if not w in stop_words])\n",
    "    return text\n",
    "\n",
    "# execute pre process for each of the docs (remove special characters)\n",
    "def create_docs(data, n_gram_max=1, tokenizer=None, train_mode=True, referance_col='name', stop_words=None):\n",
    "    df = pd.DataFrame(data=data, columns=[referance_col])\n",
    "    rare_train_words = []\n",
    "\n",
    "    # create N grams + separate punctuation from words (character N grams)\n",
    "    def add_ngram(q, n_gram_max):\n",
    "        ngrams = []\n",
    "        for n in range(2, n_gram_max + 1):\n",
    "            for w_index in range(len(q) - n + 1):\n",
    "                ngrams.append('--'.join(q[w_index:w_index + n]))\n",
    "        return q + ngrams\n",
    "    \n",
    "    # pre-process text\n",
    "    docs = []\n",
    "    for doc in df[referance_col]:\n",
    "        doc = preprocess(doc, stop_words).split()\n",
    "        docs.append(' '.join(add_ngram(doc, n_gram_max)))\n",
    "        \n",
    "    # tokenization step (why do not split on spaces?)\n",
    "    min_count = 2\n",
    "    if tokenizer is None:\n",
    "        tokenizer = Tokenizer(lower=True, filters='')\n",
    "        tokenizer.fit_on_texts(docs)\n",
    "        # summarize what was learned in tokenizer\n",
    "#         print(tokenizer.word_counts)\n",
    "#         print(tokenizer.document_count)\n",
    "#         print(tokenizer.word_index)\n",
    "#         print(tokenizer.word_docs)\n",
    "\n",
    "    if train_mode:\n",
    "        # remove low frequency words\n",
    "        num_words = sum([1 for _, v in tokenizer.word_counts.items() if v >= min_count])\n",
    "        tokenizer = Tokenizer(num_words=num_words, lower=True, filters='')\n",
    "        tokenizer.fit_on_texts(docs)\n",
    "\n",
    "    docs = tokenizer.texts_to_sequences(docs)\n",
    "    maxlen = 256\n",
    "    # cat long sentences and\n",
    "    docs = pad_sequences(sequences=docs, maxlen=maxlen)\n",
    "\n",
    "    if train_mode:\n",
    "        return docs, tokenizer\n",
    "    else:\n",
    "        return docs\n",
    "\n",
    "\n",
    "# get fsx featurew for trainig and validation set in a cross validation methodology\n",
    "def get_fasttext_features(xtrain, ytrain, xvalid, yvalid, referance_col, classes_number, all_labels, lbl_prefix='fastext_'):\n",
    "    cv_scores = []\n",
    "    pred_full_test = 0\n",
    "    print('len ytrain = ' + str(len(set(ytrain))) + \" len classes= \" + str(classes_number))\n",
    "    pred_train = np.zeros([xtrain.shape[0], classes_number])\n",
    "    \n",
    "    fsx = fasttext_classifier()\n",
    "    \n",
    "    print(\"create docs for train step (pre process, tokenization)\")\n",
    "    docstrain, tokenizer = create_docs(data=xtrain[referance_col], referance_col=referance_col)\n",
    "    fsx.set_tokenizer(tokenizer)\n",
    "    \n",
    "    print(\"create docs for validation step (pre process, tokenization)\")\n",
    "    docstest = create_docs(data=xvalid[referance_col], tokenizer=fsx.tokenizer, train_mode=False,\n",
    "                           referance_col=referance_col)\n",
    "    input_dim = np.max(docstrain) + 1\n",
    "    fsx.create_model(input_dim, classes_number=classes_number)\n",
    "\n",
    "    # split training set to 5 folds\n",
    "    kf = model_selection.KFold(n_splits=5, shuffle=True, random_state=2017)\n",
    "    cv_cnt = 1\n",
    "    for dev_index, val_index in kf.split(docstrain):\n",
    "        print(\"CV fsx:\" + str(cv_cnt))\n",
    "        cv_cnt += 1\n",
    "\n",
    "        dev_X, val_X = docstrain[dev_index], docstrain[val_index]\n",
    "        dev_y, val_y = ytrain.iloc[dev_index], ytrain.iloc[val_index]\n",
    "\n",
    "        fsx.train(dev_X, dev_y, val_X, val_y)\n",
    "        prob_val_y, cls_val_y = fsx.predict(val_X)\n",
    "        prob_test_y, cls_test_y = fsx.predict(docstest)\n",
    "\n",
    "        pred_full_test = pred_full_test + prob_test_y\n",
    "        pred_train[val_index, :] = prob_val_y\n",
    "#         try:\n",
    "#             cv_scores.append(metrics.log_loss(val_y, prob_val_y, labels=all_labels))\n",
    "#         except Exception as e:\n",
    "#             print(\"valid shape=\" + str(val_y.shape) + \"predictions shape=\" + str(prob_val_y.shape) + \"labels shape=\" + str(all_labels.shape))\n",
    "#             print(\"Oops! failed to append log_loss metrics in KFold\")\n",
    "#             print(e)\n",
    "#     try:\n",
    "#         print(\"Mean cv score : \", np.mean(cv_scores))\n",
    "#     except:\n",
    "#         print(\"error while calculating mean cv score\")\n",
    "        \n",
    "    pred_full_test = pred_full_test / 5.\n",
    "\n",
    "    columns = [lbl_prefix + str(i) for i in range(classes_number)]\n",
    "    aa = pd.DataFrame(columns=columns, data=pred_train)\n",
    "    bb = pd.DataFrame(columns=columns, data=pred_full_test)\n",
    "    return aa, bb\n",
    "\n",
    "\n",
    "# this methos to be used to save model created on training set, for new row currently not in DB\n",
    "def obtain_fasttext_model(xtrain, ytrain, xvalid, yvalid, classes_number, referance_col='name',create_doc=True, stop_words=True):\n",
    "\n",
    "    fsx = fasttext_classifier()\n",
    "    if stop_words:\n",
    "        print(\"using stop words\")\n",
    "        eng_stopwords = set(stopwords.words(\"english\"))\n",
    "    else:\n",
    "        eng_stopwords = None\n",
    "        \n",
    "    if create_doc:\n",
    "        docstrain, tokenizer = create_docs(data=xtrain[referance_col], referance_col=referance_col, stop_words=eng_stopwords)\n",
    "        fsx.set_tokenizer(tokenizer)\n",
    "        docstest = create_docs(data=xvalid[referance_col], tokenizer=fsx.tokenizer, train_mode=False,\n",
    "                           referance_col=referance_col, stop_words=eng_stopwords)\n",
    "    else:\n",
    "        docstrain=xtrain\n",
    "        docstest=xvalid\n",
    "\n",
    "    input_dim = np.max(docstrain) + 1\n",
    "    fsx.create_model(input_dim, classes_number=classes_number)\n",
    "\n",
    "    fsx.train(docstrain, ytrain, docstest, yvalid)\n",
    "    return fsx, tokenizer\n",
    "\n",
    "\n",
    "xtrain_processed = xtrain.reset_index(drop=True)\n",
    "xvalid_processed = xvalid.reset_index(drop=True)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train / Load Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "using stop words\n",
      "Train on 22576 samples, validate on 5644 samples\n",
      "Epoch 1/30\n",
      " - 83s - loss: 2.3148 - acc: 0.3355 - val_loss: 1.8296 - val_acc: 0.4818\n",
      "Epoch 2/30\n",
      " - 80s - loss: 1.6164 - acc: 0.5414 - val_loss: 1.4771 - val_acc: 0.5985\n",
      "Epoch 3/30\n",
      " - 80s - loss: 1.3496 - acc: 0.6144 - val_loss: 1.3606 - val_acc: 0.6332\n",
      "Epoch 4/30\n",
      " - 79s - loss: 1.1865 - acc: 0.6603 - val_loss: 1.3060 - val_acc: 0.6492\n",
      "Epoch 5/30\n",
      " - 82s - loss: 1.0699 - acc: 0.6933 - val_loss: 1.2794 - val_acc: 0.6582\n",
      "Epoch 6/30\n",
      " - 82s - loss: 0.9794 - acc: 0.7152 - val_loss: 1.2582 - val_acc: 0.6637\n",
      "Epoch 7/30\n",
      " - 79s - loss: 0.9092 - acc: 0.7372 - val_loss: 1.2545 - val_acc: 0.6706\n",
      "Epoch 8/30\n",
      " - 80s - loss: 0.8559 - acc: 0.7454 - val_loss: 1.2642 - val_acc: 0.6743\n",
      "Epoch 9/30\n",
      " - 80s - loss: 0.7993 - acc: 0.7628 - val_loss: 1.2743 - val_acc: 0.6774\n",
      "Epoch 10/30\n",
      " - 81s - loss: 0.7574 - acc: 0.7734 - val_loss: 1.2776 - val_acc: 0.6779\n",
      "Epoch 11/30\n"
     ]
    }
   ],
   "source": [
    "train = True\n",
    "\n",
    "if train:\n",
    "    fsx, tokenizer = obtain_fasttext_model(xtrain_processed, ytrain, xvalid_processed, yvalid, classes_number,\n",
    "                                                             referance_col='name', stop_words=True)\n",
    "    print(\"finish training FT classifier\")\n",
    "    # export model and tokenizer\n",
    "    pickle.dump(tokenizer, open(tokenizer_file_name, \"wb\"), protocol=pickle.HIGHEST_PROTOCOL)\n",
    "    fsx.model.save(classifier_file_name)\n",
    "else:\n",
    "    # load model from disk\n",
    "    import pickle\n",
    "    from keras.models import load_model\n",
    "    import pandas as pd\n",
    "    loaded_tokenizer = pickle.load(open(tokenizer_file_name, \"rb\"))\n",
    "    loaded_fsx = load_model(classifier_file_name)\n",
    "\n",
    "\n",
    "print(\"Exported model and tokenizer\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# predict team by string\n",
    "string_to_predict = 'pipeline module does not work properly'\n",
    "column = pd.DataFrame({'name' : [string_to_predict]})\n",
    "docs = create_docs(data=column, train_mode=False, tokenizer=loaded_tokenizer, referance_col='name')\n",
    "prob_val_matrix = loaded_fsx.predict(docs)\n",
    "prob_val_y = prob_val_matrix[0]\n",
    "max_teams = prob_val_y.argsort()[-2:][::-1]\n",
    "print (str(prob_val_y[max_teams[0]]) + \" probability it's: \" + team_id_to_name_map[le.inverse_transform([max_teams[0]])[0]])\n",
    "print (str(prob_val_y[max_teams[1]]) + \" probability it's: \" + team_id_to_name_map[le.inverse_transform([max_teams[1]])[0]])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # extract NLP features\n",
    "# from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "# from sklearn.pipeline import Pipeline, FeatureUnion\n",
    "# from sklearn.decomposition import PCA as sklearnPCA, TruncatedSVD\n",
    "# from sklearn.base import BaseEstimator, TransformerMixin\n",
    "# class ItemSelector(BaseEstimator, TransformerMixin):\n",
    "\n",
    "#     def __init__(self, key):\n",
    "#         self.key = key\n",
    "\n",
    "#     def fit(self, x, y=None):\n",
    "#         return self\n",
    "\n",
    "#     def transform(self, data_dict):\n",
    "#         return data_dict[self.key]\n",
    "    \n",
    "# tf_idf_3_grams = Pipeline([\n",
    "#                 ('sel', ItemSelector(key='name')),\n",
    "#                 ('tf', TfidfVectorizer(max_features=1000,\n",
    "#                                        strip_accents='unicode', token_pattern=r'\\w{1,}',\n",
    "#                                        ngram_range=(1, 3), use_idf=1, smooth_idf=1, sublinear_tf=1,\n",
    "#                                        stop_words='english')),\n",
    "#                 ('svd', TruncatedSVD(n_components=50))\n",
    "#     ])    \n",
    "# name_features = tf_idf_3_grams.fit_transform(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# train test split: train, valid, test sets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## regularization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "how to avoid overfitting\n",
    "- reduce layers\n",
    "- randomly neglect nodes from producing output in nn (dropout)\n",
    "- add regularization (penelize for large weights: loss + x) L2 regularization\n",
    "\n",
    "how to avoid underfitting - \n",
    "- encrease layers / nodes.\n",
    "- add additional features.\n",
    "\n",
    "to add regularization to layer:\n",
    "model.add(Dense(5, input_shape=(26,), kernal.regulizer=regulizers.l2(0.01), activation='relu'))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# create the model with layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "# Dense is a type of layer (basic type), fully connected layer\n",
    "# first arg is number of neurons in layer\n",
    "# the activation function getting the wieghted sum from all input nodes, and output a number between 0 to 1.\n",
    "# the first layer is hidden layer (and the input defined with param input_shape)\n",
    "model.add(Dense(5, input_shape=(50,), activation='relu'))\n",
    "model.add(Dense(5, activation='relu'))\n",
    "model.add(Dense(1, activation='softmax'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# compile the module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# using optimizer (SGC, Adma..) , minimize the loss funtion.\n",
    "# Adam is a variation of SGC, also choose loss func + metrics (printed out)\n",
    "model.compile(Adam(lr=.0001), loss='binary_crossentropy', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# train the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# expect to get numpy array\n",
    "# verbose: how much output we want to see \n",
    "model.fit(train_samples, train_labels, validation_split=0.20, batch_size=10, epochs=20, shuffle=True, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
