{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "in order to install NLTK stop words, run the following python commands -\n",
    "\n",
    "import nltk\n",
    "nltk.download()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Predict team using defect title"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "in the following notebook, a model is built and trained using defects data from center. \n",
    "for predicting one field from defects form - team field."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Required libraries\n",
    "general imports - keras, sklearn, numpy, pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "import keras \n",
    "from keras.models import Sequential\n",
    "from keras.layers import Activation\n",
    "from keras.optimizers import Adam\n",
    "from keras import regularizers\n",
    "from keras.layers import Dense, GlobalAveragePooling1D, Embedding, Dropout, Conv1D, MaxPooling1D, Flatten\n",
    "from keras.callbacks import EarlyStopping\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from sklearn import model_selection\n",
    "from sklearn import metrics\n",
    "from keras.metrics import categorical_crossentropy\n",
    "from sklearn.model_selection import train_test_split\n",
    "import pandas as pd\n",
    "import json\n",
    "import numpy as np\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# load data from dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are: 28970 records of defects in dataset.\n"
     ]
    }
   ],
   "source": [
    "# Reads and converts json / Excel format to python dict.\n",
    "def js_r(data):\n",
    "   with open(data, encoding='utf-8') as f_in:\n",
    "       return(json.load(f_in))\n",
    "\n",
    "def read_excel(path):\n",
    "    return pd.read_excel(open(path, 'rb'),\n",
    "                  sheet_name='Sheet2')  # doctest: +SKIP\n",
    "\n",
    "# load JSON data\n",
    "# data_path=r'C:\\dev\\defects.json'\n",
    "# my_dic_data = js_r(data_path)['data']\n",
    "\n",
    "# Load EXCEL data:\n",
    "data_path = r'datasets' + os.sep + 'defects-all-spread.xlsx'\n",
    "my_dic_data = read_excel(data_path)\n",
    "print('There are: ' + str(len(my_dic_data)) + ' records of defects in dataset.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Transform & normalize dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "handling ids fields from json dataset, remove unused columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "def transform_data(data_list):\n",
    "    print(len(data_list))\n",
    "    for defect in my_dic_data:\n",
    "        for column in defect.keys():\n",
    "            if not defect[column] == None and type(defect[column]) is dict and 'id' in defect[column]:\n",
    "                defect[column] = defect[column]['id']\n",
    "        \n",
    "    return pd.DataFrame(my_dic_data).drop(['access_granted', 'blocked_reason', 'path', 'phase',\n",
    "                                           'blocked', 'original_id', 'priority', 'defect_root_level', 'story_points', 'user_tags', 'program', 'taxonomies', 'version_stamp', 'detected_in_build', 'sprint', 'dependency_problem_type', 'defect_type'], axis=1)\n",
    "# df = transform_data(my_dic_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "cleaning the dataset (removing defects without labels, defects with only one label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "An example of dataset structure:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>creation_time</th>\n",
       "      <th>id</th>\n",
       "      <th>name</th>\n",
       "      <th>description</th>\n",
       "      <th>knowledge_modified_udf</th>\n",
       "      <th>parent.name</th>\n",
       "      <th>parent.id</th>\n",
       "      <th>release.id</th>\n",
       "      <th>team.id</th>\n",
       "      <th>team.name</th>\n",
       "      <th>product_areas.id</th>\n",
       "      <th>product_areas.name</th>\n",
       "      <th>application_modules.id</th>\n",
       "      <th>application_modules.name</th>\n",
       "      <th>qa_owner.id</th>\n",
       "      <th>qa_owner.name</th>\n",
       "      <th>owner.id</th>\n",
       "      <th>owner.name</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2017-01-11 09:05:12</td>\n",
       "      <td>205022</td>\n",
       "      <td>[Regression Day - Firefox] - Exception thrown ...</td>\n",
       "      <td>&amp;lt;html&amp;gt;&amp;lt;body&amp;gt;&amp;lt;p&amp;gt;1. create tes...</td>\n",
       "      <td>NaT</td>\n",
       "      <td>Requirement Backlog</td>\n",
       "      <td>1001</td>\n",
       "      <td>22001.0</td>\n",
       "      <td>3002.0</td>\n",
       "      <td>OMG Yuval (Do Not Use)</td>\n",
       "      <td>78018.0</td>\n",
       "      <td>10 Tests</td>\n",
       "      <td>78018.0</td>\n",
       "      <td>10 Tests</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>9001.0</td>\n",
       "      <td>sari.bivas@microfocus.com</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2016-12-27 10:06:39</td>\n",
       "      <td>198001</td>\n",
       "      <td>Bug Hunt 12.53.19 - when add \"On it\" user I ge...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaT</td>\n",
       "      <td>Requirement Backlog</td>\n",
       "      <td>1001</td>\n",
       "      <td>22001.0</td>\n",
       "      <td>8003.0</td>\n",
       "      <td>Sharon - XMen Magneto</td>\n",
       "      <td>79021.0</td>\n",
       "      <td>Failure Analysis</td>\n",
       "      <td>79021.0</td>\n",
       "      <td>Failure Analysis</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>3035.0</td>\n",
       "      <td>abed.masrawa@hpe.com</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        creation_time      id  \\\n",
       "0 2017-01-11 09:05:12  205022   \n",
       "1 2016-12-27 10:06:39  198001   \n",
       "\n",
       "                                                name  \\\n",
       "0  [Regression Day - Firefox] - Exception thrown ...   \n",
       "1  Bug Hunt 12.53.19 - when add \"On it\" user I ge...   \n",
       "\n",
       "                                         description knowledge_modified_udf  \\\n",
       "0  &lt;html&gt;&lt;body&gt;&lt;p&gt;1. create tes...                    NaT   \n",
       "1                                                NaN                    NaT   \n",
       "\n",
       "           parent.name  parent.id  release.id  team.id  \\\n",
       "0  Requirement Backlog       1001     22001.0   3002.0   \n",
       "1  Requirement Backlog       1001     22001.0   8003.0   \n",
       "\n",
       "                team.name  product_areas.id product_areas.name  \\\n",
       "0  OMG Yuval (Do Not Use)           78018.0           10 Tests   \n",
       "1   Sharon - XMen Magneto           79021.0   Failure Analysis   \n",
       "\n",
       "   application_modules.id application_modules.name  qa_owner.id qa_owner.name  \\\n",
       "0                 78018.0                 10 Tests          NaN           NaN   \n",
       "1                 79021.0         Failure Analysis          NaN           NaN   \n",
       "\n",
       "   owner.id                 owner.name  \n",
       "0    9001.0  sari.bivas@microfocus.com  \n",
       "1    3035.0       abed.masrawa@hpe.com  "
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# build dataframe from dict\n",
    "df = pd.DataFrame(my_dic_data)\n",
    "# cleaning rows without labels OR descritpion\n",
    "df = df[df['team.id'].map(lambda x: x != None)]\n",
    "df = df[df['name'].map(lambda x: x != None)]\n",
    "train_df = df[df['description'].map(lambda x: x != None)]\n",
    "train_df = train_df.groupby('team.id').filter(lambda x : len(x)>2)\n",
    "\n",
    "# building translation map from team id -> team name\n",
    "team_id_to_name_map = pd.Series(df['team.name'].values,index=df['team.id']).to_dict()\n",
    "# spliting Lables from features\n",
    "X = train_df.drop(['team.id'], axis=1)\n",
    "Y = train_df['team.id']\n",
    "\n",
    "print(\"An example of dataset structure:\")\n",
    "train_df.head(n=2)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "labels encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "there are: 34 different labels in dataset, which corresponds to all teams\n"
     ]
    }
   ],
   "source": [
    "# label envoding\n",
    "from sklearn import preprocessing\n",
    "le = preprocessing.LabelEncoder()\n",
    "encoded_labels = pd.DataFrame(le.fit_transform(train_df['team.id']))\n",
    "classes_number = encoded_labels.groupby(0).nunique().shape[0]\n",
    "all_labels = encoded_labels[0].unique()\n",
    "print('there are: ' + str(classes_number) + ' different labels in dataset, which corresponds to all teams')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train samples: (22576, 17) validation samples: (5644, 17)\n"
     ]
    }
   ],
   "source": [
    "# ensure that the model is not overfitting \n",
    "# train & validation tests with labels, test without\n",
    "xtrain, xvalid, ytrain, yvalid = train_test_split(X, encoded_labels,\n",
    "                                                      stratify=encoded_labels,\n",
    "                                                      random_state=42,\n",
    "                                                      test_size=0.2, shuffle=True)\n",
    "\n",
    "\n",
    "print ('train samples: ' + str(xtrain.shape) + ' validation samples: ' + str(xvalid.shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Build Model + Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import metrics\n",
    "# parameters:BATCH_SIZE=16, EPOC_SIZE=30\n",
    "BATCH_SIZE = 50 # smaller batch size consume less memory (but can decrease accuracy)\n",
    "EPOC_SIZE = 30\n",
    "tokenizer_file_name = \"models\" + os.sep + \"FTtokenizer_\" + str(BATCH_SIZE) + \"B_\" + str(EPOC_SIZE) + \"E\" + \".pkl\"\n",
    "classifier_file_name = \"models\" + os.sep + \"FTClassifier_\" + str(BATCH_SIZE) + \"B_\" + str(EPOC_SIZE) + \"E\" + \".pkl\"\n",
    "\n",
    "class fasttext_classifier(object):\n",
    "    def __init__(self):\n",
    "        self.train_df = None\n",
    "        self.train_X = None\n",
    "        self.train_Y = None\n",
    "        self.vslid_X = None\n",
    "        self.valid_Y = None\n",
    "        self.model = None\n",
    "        self.hist = None\n",
    "        self.tokenizer = None\n",
    "        self.rare_train_words = []\n",
    "    \n",
    "    def create_model(self, input_dim, classes_number, embedding_dims=32, optimizer='adam'):\n",
    "        self.labels = all_labels\n",
    "        self.model = Sequential()\n",
    "        \n",
    "        # we start off with an efficient embedding layer which maps\n",
    "        # our vocab indices into embedding_dims dimensions\n",
    "        self.model.add(Embedding(input_dim=input_dim, output_dim=embedding_dims, input_length=256))\n",
    "        # Dropout consists in randomly setting a fraction rate of input units to 0 at each update during training time,\n",
    "        # which helps prevent overfitting.\n",
    "        self.model.add(Dropout(0.3))\n",
    "        #This layer creates a convolution kernel that is convolved with the layer input over a single spatial (or temporal) dimension to produce a tensor of outputs. If use_bias is True, a bias vector is created and added to the outputs. Finally, if activation is not None, it is applied to the outputs as well.\n",
    "        self.model.add(Conv1D(64,\n",
    "                              5,\n",
    "                              padding='valid',\n",
    "                              activation='relu'))\n",
    "        self.model.add(Dropout(0.3))\n",
    "        self.model.add(MaxPooling1D())\n",
    "        self.model.add(Flatten())\n",
    "        self.model.add(Dense(800, activation='relu'))\n",
    "        self.model.add(Dropout(0.5))\n",
    "        self.model.add(Dense(classes_number, activation='softmax'))\n",
    "\n",
    "        self.model.compile(loss='sparse_categorical_crossentropy',\n",
    "                           optimizer=optimizer,\n",
    "                           metrics=['accuracy'])\n",
    "\n",
    "        return\n",
    "\n",
    "    def set_tokenizer(self, tokenizer):\n",
    "        self.tokenizer = tokenizer\n",
    "\n",
    "    def train(self, docstrain, ytrain, docsvalid, yvalid):\n",
    "        self.train_X = docstrain\n",
    "        self.train_Y = ytrain\n",
    "        self.hist = self.model.fit(docstrain, ytrain,\n",
    "                                   batch_size=BATCH_SIZE,\n",
    "                                   validation_data=(docsvalid, yvalid),\n",
    "                                   epochs=EPOC_SIZE, verbose=2,\n",
    "                                   callbacks=[EarlyStopping(patience=4, monitor='val_loss')])\n",
    "\n",
    "        predictions = self.model.predict_proba(docsvalid)\n",
    "        predictions_classes = self.model.predict_classes(docsvalid)\n",
    "        try:\n",
    "            # print(\"valid shape=\" + str(yvalid.shape) + \"predictions shape=\" + str(predictions.shape) + \"labels shape=\" + str(self.labels.shape))\n",
    "            print(\"accuracy on validation set after training: %0.3f\" % (\n",
    "                np.sum(predictions_classes == yvalid[0]) / len(yvalid[0])))\n",
    "#             print(\"prediction rows: \" + str(len(predictions.values)))\n",
    "#             print(\"prediction columns: \" + str(len(predictions.values[0])))\n",
    "#             print(\"fasttext logloss: %0.3f \" % metrics.log_loss(yvalid[0].values, predictions.values, labels=self.labels[0].values))\n",
    "        except Exception as e:\n",
    "            print(\"Oops!! occured. could not calculate metrics for this epoc\")\n",
    "            print(e)\n",
    "        return\n",
    "\n",
    "    def predict(self, docstest):\n",
    "        print(\"run prediction\")\n",
    "        predictions = self.model.predict_proba(docstest)\n",
    "        predictions_classes = self.model.predict_classes(docstest)\n",
    "        return predictions, predictions_classes\n",
    "\n",
    "    def plot_train_vs_val(self):\n",
    "        hist = self.model.history\n",
    "        hist_dict = hist.history\n",
    "        # plot loss\n",
    "        fig = plt.figure()\n",
    "        plt.subplot(211)\n",
    "        val_loss = hist_dict.get('val_loss')\n",
    "        val_loss_line = plt.plot(val_loss, label='val_loss')\n",
    "        plt.legend()\n",
    "        loss = hist_dict.get('loss')\n",
    "        plt.plot(loss, label='train_loss')\n",
    "        plt.legend()\n",
    "        plt.title(\"train and validation loss\")\n",
    "        plt.ylabel(\"loss\")\n",
    "\n",
    "        # plot accuracy\n",
    "        plt.subplot(212)\n",
    "        val_acc = hist_dict.get('val_acc')\n",
    "        plt.plot(val_acc, label='val_acc')\n",
    "        plt.legend()\n",
    "        acc = hist_dict.get('acc')\n",
    "        plt.plot(acc, label='train_acc')\n",
    "        plt.legend()\n",
    "        plt.title(\"train and validation accuracy\")\n",
    "        plt.ylabel(\"accuracy\")\n",
    "        plt.xlabel(\"step\")\n",
    "\n",
    "        fig.savefig(\"fast-text-itr-performance.pdf\", format='pdf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.layers import Dense, GlobalAveragePooling1D, Embedding\n",
    "from keras.models import Sequential\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from sklearn import model_selection\n",
    "from sklearn import metrics\n",
    "from nltk.corpus import stopwords\n",
    "import pickle\n",
    "\n",
    "# preproceeings are:\n",
    "# Separate punctuation from words\n",
    "# Remove lower frequency words ( <= 2)\n",
    "# Cut a longer document which contains 256 words\n",
    "def preprocess(text, stop_words):\n",
    "    text = text.replace(\"' \", \" ' \")\n",
    "    signs = set(',.:;\"?![]')\n",
    "    prods = set(text) & signs\n",
    "    \n",
    "    if not prods:\n",
    "        return text\n",
    "\n",
    "    for sign in prods:\n",
    "        text = text.replace(sign, ' {} '.format(sign))\n",
    "    \n",
    "    if stop_words is not None:\n",
    "        # Remove Stopwords\n",
    "        text = ' '.join([w for w in text.split(' ') if not w in stop_words])\n",
    "    return text\n",
    "\n",
    "# execute pre process for each of the docs (remove special characters)\n",
    "def create_docs(data, n_gram_max=1, tokenizer=None, train_mode=True, referance_col='name', stop_words=None):\n",
    "    df = pd.DataFrame(data=data, columns=[referance_col])\n",
    "    rare_train_words = []\n",
    "\n",
    "    # create N grams + separate punctuation from words (character N grams)\n",
    "    def add_ngram(q, n_gram_max):\n",
    "        ngrams = []\n",
    "        for n in range(2, n_gram_max + 1):\n",
    "            for w_index in range(len(q) - n + 1):\n",
    "                ngrams.append('--'.join(q[w_index:w_index + n]))\n",
    "        return q + ngrams\n",
    "    \n",
    "    # pre-process text\n",
    "    docs = []\n",
    "    for doc in df[referance_col]:\n",
    "        doc = preprocess(doc, stop_words).split()\n",
    "        docs.append(' '.join(add_ngram(doc, n_gram_max)))\n",
    "        \n",
    "    # tokenization step (why do not split on spaces?)\n",
    "    min_count = 2\n",
    "    if tokenizer is None:\n",
    "        tokenizer = Tokenizer(lower=True, filters='')\n",
    "        tokenizer.fit_on_texts(docs)\n",
    "        # summarize what was learned in tokenizer\n",
    "#         print(tokenizer.word_counts)\n",
    "#         print(tokenizer.document_count)\n",
    "#         print(tokenizer.word_index)\n",
    "#         print(tokenizer.word_docs)\n",
    "\n",
    "    if train_mode:\n",
    "        # remove low frequency words\n",
    "        num_words = sum([1 for _, v in tokenizer.word_counts.items() if v >= min_count])\n",
    "        tokenizer = Tokenizer(num_words=num_words, lower=True, filters='')\n",
    "        tokenizer.fit_on_texts(docs)\n",
    "\n",
    "    docs = tokenizer.texts_to_sequences(docs)\n",
    "    maxlen = 256\n",
    "    # cat long sentences and\n",
    "    docs = pad_sequences(sequences=docs, maxlen=maxlen)\n",
    "\n",
    "    if train_mode:\n",
    "        return docs, tokenizer\n",
    "    else:\n",
    "        return docs\n",
    "\n",
    "\n",
    "# get fsx featurew for trainig and validation set in a cross validation methodology\n",
    "def get_fasttext_features(xtrain, ytrain, xvalid, yvalid, referance_col, classes_number, all_labels, lbl_prefix='fastext_'):\n",
    "    cv_scores = []\n",
    "    pred_full_test = 0\n",
    "    print('len ytrain = ' + str(len(set(ytrain))) + \" len classes= \" + str(classes_number))\n",
    "    pred_train = np.zeros([xtrain.shape[0], classes_number])\n",
    "    \n",
    "    fsx = fasttext_classifier()\n",
    "    \n",
    "    print(\"create docs for train step (pre process, tokenization)\")\n",
    "    docstrain, tokenizer = create_docs(data=xtrain[referance_col], referance_col=referance_col)\n",
    "    fsx.set_tokenizer(tokenizer)\n",
    "    \n",
    "    print(\"create docs for validation step (pre process, tokenization)\")\n",
    "    docstest = create_docs(data=xvalid[referance_col], tokenizer=fsx.tokenizer, train_mode=False,\n",
    "                           referance_col=referance_col)\n",
    "    input_dim = np.max(docstrain) + 1\n",
    "    fsx.create_model(input_dim, classes_number=classes_number)\n",
    "\n",
    "    # split training set to 5 folds\n",
    "    kf = model_selection.KFold(n_splits=5, shuffle=True, random_state=2017)\n",
    "    cv_cnt = 1\n",
    "    for dev_index, val_index in kf.split(docstrain):\n",
    "        print(\"CV fsx:\" + str(cv_cnt))\n",
    "        cv_cnt += 1\n",
    "\n",
    "        dev_X, val_X = docstrain[dev_index], docstrain[val_index]\n",
    "        dev_y, val_y = ytrain.iloc[dev_index], ytrain.iloc[val_index]\n",
    "\n",
    "        fsx.train(dev_X, dev_y, val_X, val_y)\n",
    "        prob_val_y, cls_val_y = fsx.predict(val_X)\n",
    "        prob_test_y, cls_test_y = fsx.predict(docstest)\n",
    "\n",
    "        pred_full_test = pred_full_test + prob_test_y\n",
    "        pred_train[val_index, :] = prob_val_y\n",
    "#         try:\n",
    "#             cv_scores.append(metrics.log_loss(val_y, prob_val_y, labels=all_labels))\n",
    "#         except Exception as e:\n",
    "#             print(\"valid shape=\" + str(val_y.shape) + \"predictions shape=\" + str(prob_val_y.shape) + \"labels shape=\" + str(all_labels.shape))\n",
    "#             print(\"Oops! failed to append log_loss metrics in KFold\")\n",
    "#             print(e)\n",
    "#     try:\n",
    "#         print(\"Mean cv score : \", np.mean(cv_scores))\n",
    "#     except:\n",
    "#         print(\"error while calculating mean cv score\")\n",
    "        \n",
    "    pred_full_test = pred_full_test / 5.\n",
    "\n",
    "    columns = [lbl_prefix + str(i) for i in range(classes_number)]\n",
    "    aa = pd.DataFrame(columns=columns, data=pred_train)\n",
    "    bb = pd.DataFrame(columns=columns, data=pred_full_test)\n",
    "    return aa, bb\n",
    "\n",
    "\n",
    "# this methos to be used to save model created on training set, for new row currently not in DB\n",
    "def obtain_fasttext_model(xtrain, ytrain, xvalid, yvalid, classes_number, referance_col='name',create_doc=True, stop_words=True):\n",
    "\n",
    "    fsx = fasttext_classifier()\n",
    "    if stop_words:\n",
    "        print(\"using stop words\")\n",
    "        eng_stopwords = set(stopwords.words(\"english\"))\n",
    "    else:\n",
    "        eng_stopwords = None\n",
    "        \n",
    "    if create_doc:\n",
    "        docstrain, tokenizer = create_docs(data=xtrain[referance_col], referance_col=referance_col, stop_words=eng_stopwords)\n",
    "        fsx.set_tokenizer(tokenizer)\n",
    "        docstest = create_docs(data=xvalid[referance_col], tokenizer=fsx.tokenizer, train_mode=False,\n",
    "                           referance_col=referance_col, stop_words=eng_stopwords)\n",
    "    else:\n",
    "        docstrain=xtrain\n",
    "        docstest=xvalid\n",
    "\n",
    "    input_dim = np.max(docstrain) + 1\n",
    "    fsx.create_model(input_dim, classes_number=classes_number)\n",
    "\n",
    "    fsx.train(docstrain, ytrain, docstest, yvalid)\n",
    "    return fsx, tokenizer\n",
    "\n",
    "\n",
    "xtrain_processed = xtrain.reset_index(drop=True)\n",
    "xvalid_processed = xvalid.reset_index(drop=True)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train / Load Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "using stop words\n",
      "Train on 22576 samples, validate on 5644 samples\n",
      "Epoch 1/30\n",
      " - 83s - loss: 2.3148 - acc: 0.3355 - val_loss: 1.8296 - val_acc: 0.4818\n",
      "Epoch 2/30\n",
      " - 80s - loss: 1.6164 - acc: 0.5414 - val_loss: 1.4771 - val_acc: 0.5985\n",
      "Epoch 3/30\n",
      " - 80s - loss: 1.3496 - acc: 0.6144 - val_loss: 1.3606 - val_acc: 0.6332\n",
      "Epoch 4/30\n",
      " - 79s - loss: 1.1865 - acc: 0.6603 - val_loss: 1.3060 - val_acc: 0.6492\n",
      "Epoch 5/30\n",
      " - 82s - loss: 1.0699 - acc: 0.6933 - val_loss: 1.2794 - val_acc: 0.6582\n",
      "Epoch 6/30\n",
      " - 82s - loss: 0.9794 - acc: 0.7152 - val_loss: 1.2582 - val_acc: 0.6637\n",
      "Epoch 7/30\n",
      " - 79s - loss: 0.9092 - acc: 0.7372 - val_loss: 1.2545 - val_acc: 0.6706\n",
      "Epoch 8/30\n",
      " - 80s - loss: 0.8559 - acc: 0.7454 - val_loss: 1.2642 - val_acc: 0.6743\n",
      "Epoch 9/30\n",
      " - 80s - loss: 0.7993 - acc: 0.7628 - val_loss: 1.2743 - val_acc: 0.6774\n",
      "Epoch 10/30\n",
      " - 81s - loss: 0.7574 - acc: 0.7734 - val_loss: 1.2776 - val_acc: 0.6779\n",
      "Epoch 11/30\n",
      " - 80s - loss: 0.7224 - acc: 0.7814 - val_loss: 1.2799 - val_acc: 0.6729\n",
      "accuracy on validation set after training: 0.673\n",
      "finish training FT classifier\n",
      "Exported model and tokenizer\n"
     ]
    }
   ],
   "source": [
    "train = True\n",
    "\n",
    "saver = tf.train.Saver()\n",
    "sess = keras.backend.get_session()\n",
    "saver.save(sess, './keras_model')\n",
    "\n",
    "if train:\n",
    "    fsx, tokenizer = obtain_fasttext_model(xtrain_processed, ytrain, xvalid_processed, yvalid, classes_number,\n",
    "                                                             referance_col='name', stop_words=True)\n",
    "    print(\"finish training FT classifier\")\n",
    "    # export model and tokenizer\n",
    "    pickle.dump(tokenizer, open(tokenizer_file_name, \"wb\"), protocol=pickle.HIGHEST_PROTOCOL)\n",
    "    fsx.model.save(classifier_file_name)\n",
    "else:\n",
    "    # load model from disk\n",
    "    import pickle\n",
    "    from keras.models import load_model\n",
    "    import pandas as pd\n",
    "    loaded_tokenizer = pickle.load(open(tokenizer_file_name, \"rb\"))\n",
    "    loaded_fsx = load_model(classifier_file_name)\n",
    "\n",
    "\n",
    "print(\"Exported model and tokenizer\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "ename": "UnknownError",
     "evalue": "Failed to rename: keras_model.data-00000-of-00001.tempstate16543772219696919801 to: keras_model.data-00000-of-00001 : Access is denied.\r\n; Input/output error\n\t [[node save_1/SaveV2 (defined at <ipython-input-113-1259041555ac>:3) ]]\n\nCaused by op 'save_1/SaveV2', defined at:\n  File \"C:\\Users\\vaingato.CORPDOM\\AppData\\Local\\Continuum\\anaconda3\\lib\\runpy.py\", line 193, in _run_module_as_main\n    \"__main__\", mod_spec)\n  File \"C:\\Users\\vaingato.CORPDOM\\AppData\\Local\\Continuum\\anaconda3\\lib\\runpy.py\", line 85, in _run_code\n    exec(code, run_globals)\n  File \"C:\\Users\\vaingato.CORPDOM\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\ipykernel_launcher.py\", line 16, in <module>\n    app.launch_new_instance()\n  File \"C:\\Users\\vaingato.CORPDOM\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\traitlets\\config\\application.py\", line 658, in launch_instance\n    app.start()\n  File \"C:\\Users\\vaingato.CORPDOM\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\ipykernel\\kernelapp.py\", line 505, in start\n    self.io_loop.start()\n  File \"C:\\Users\\vaingato.CORPDOM\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\tornado\\platform\\asyncio.py\", line 148, in start\n    self.asyncio_loop.run_forever()\n  File \"C:\\Users\\vaingato.CORPDOM\\AppData\\Local\\Continuum\\anaconda3\\lib\\asyncio\\base_events.py\", line 539, in run_forever\n    self._run_once()\n  File \"C:\\Users\\vaingato.CORPDOM\\AppData\\Local\\Continuum\\anaconda3\\lib\\asyncio\\base_events.py\", line 1775, in _run_once\n    handle._run()\n  File \"C:\\Users\\vaingato.CORPDOM\\AppData\\Local\\Continuum\\anaconda3\\lib\\asyncio\\events.py\", line 88, in _run\n    self._context.run(self._callback, *self._args)\n  File \"C:\\Users\\vaingato.CORPDOM\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\tornado\\ioloop.py\", line 690, in <lambda>\n    lambda f: self._run_callback(functools.partial(callback, future))\n  File \"C:\\Users\\vaingato.CORPDOM\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\tornado\\ioloop.py\", line 743, in _run_callback\n    ret = callback()\n  File \"C:\\Users\\vaingato.CORPDOM\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\tornado\\gen.py\", line 781, in inner\n    self.run()\n  File \"C:\\Users\\vaingato.CORPDOM\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\tornado\\gen.py\", line 742, in run\n    yielded = self.gen.send(value)\n  File \"C:\\Users\\vaingato.CORPDOM\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 357, in process_one\n    yield gen.maybe_future(dispatch(*args))\n  File \"C:\\Users\\vaingato.CORPDOM\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\tornado\\gen.py\", line 209, in wrapper\n    yielded = next(result)\n  File \"C:\\Users\\vaingato.CORPDOM\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 267, in dispatch_shell\n    yield gen.maybe_future(handler(stream, idents, msg))\n  File \"C:\\Users\\vaingato.CORPDOM\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\tornado\\gen.py\", line 209, in wrapper\n    yielded = next(result)\n  File \"C:\\Users\\vaingato.CORPDOM\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 534, in execute_request\n    user_expressions, allow_stdin,\n  File \"C:\\Users\\vaingato.CORPDOM\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\tornado\\gen.py\", line 209, in wrapper\n    yielded = next(result)\n  File \"C:\\Users\\vaingato.CORPDOM\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\ipykernel\\ipkernel.py\", line 294, in do_execute\n    res = shell.run_cell(code, store_history=store_history, silent=silent)\n  File \"C:\\Users\\vaingato.CORPDOM\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\ipykernel\\zmqshell.py\", line 536, in run_cell\n    return super(ZMQInteractiveShell, self).run_cell(*args, **kwargs)\n  File \"C:\\Users\\vaingato.CORPDOM\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 2848, in run_cell\n    raw_cell, store_history, silent, shell_futures)\n  File \"C:\\Users\\vaingato.CORPDOM\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 2874, in _run_cell\n    return runner(coro)\n  File \"C:\\Users\\vaingato.CORPDOM\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\IPython\\core\\async_helpers.py\", line 67, in _pseudo_sync_runner\n    coro.send(None)\n  File \"C:\\Users\\vaingato.CORPDOM\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3049, in run_cell_async\n    interactivity=interactivity, compiler=compiler, result=result)\n  File \"C:\\Users\\vaingato.CORPDOM\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3214, in run_ast_nodes\n    if (yield from self.run_code(code, result)):\n  File \"C:\\Users\\vaingato.CORPDOM\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3296, in run_code\n    exec(code_obj, self.user_global_ns, self.user_ns)\n  File \"<ipython-input-113-1259041555ac>\", line 3, in <module>\n    saver = tf.train.Saver()\n  File \"C:\\Users\\vaingato.CORPDOM\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\tensorflow\\python\\training\\saver.py\", line 832, in __init__\n    self.build()\n  File \"C:\\Users\\vaingato.CORPDOM\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\tensorflow\\python\\training\\saver.py\", line 844, in build\n    self._build(self._filename, build_save=True, build_restore=True)\n  File \"C:\\Users\\vaingato.CORPDOM\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\tensorflow\\python\\training\\saver.py\", line 881, in _build\n    build_save=build_save, build_restore=build_restore)\n  File \"C:\\Users\\vaingato.CORPDOM\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\tensorflow\\python\\training\\saver.py\", line 510, in _build_internal\n    save_tensor = self._AddSaveOps(filename_tensor, saveables)\n  File \"C:\\Users\\vaingato.CORPDOM\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\tensorflow\\python\\training\\saver.py\", line 210, in _AddSaveOps\n    save = self.save_op(filename_tensor, saveables)\n  File \"C:\\Users\\vaingato.CORPDOM\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\tensorflow\\python\\training\\saver.py\", line 124, in save_op\n    tensors)\n  File \"C:\\Users\\vaingato.CORPDOM\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\tensorflow\\python\\ops\\gen_io_ops.py\", line 1920, in save_v2\n    name=name)\n  File \"C:\\Users\\vaingato.CORPDOM\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\op_def_library.py\", line 788, in _apply_op_helper\n    op_def=op_def)\n  File \"C:\\Users\\vaingato.CORPDOM\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\tensorflow\\python\\util\\deprecation.py\", line 507, in new_func\n    return func(*args, **kwargs)\n  File \"C:\\Users\\vaingato.CORPDOM\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\ops.py\", line 3300, in create_op\n    op_def=op_def)\n  File \"C:\\Users\\vaingato.CORPDOM\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\ops.py\", line 1801, in __init__\n    self._traceback = tf_stack.extract_stack()\n\nUnknownError (see above for traceback): Failed to rename: keras_model.data-00000-of-00001.tempstate16543772219696919801 to: keras_model.data-00000-of-00001 : Access is denied.\r\n; Input/output error\n\t [[node save_1/SaveV2 (defined at <ipython-input-113-1259041555ac>:3) ]]\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mUnknownError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_do_call\u001b[1;34m(self, fn, *args)\u001b[0m\n\u001b[0;32m   1333\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1334\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1335\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[1;34m(feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[0;32m   1318\u001b[0m       return self._call_tf_sessionrun(\n\u001b[1;32m-> 1319\u001b[1;33m           options, feed_dict, fetch_list, target_list, run_metadata)\n\u001b[0m\u001b[0;32m   1320\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_call_tf_sessionrun\u001b[1;34m(self, options, feed_dict, fetch_list, target_list, run_metadata)\u001b[0m\n\u001b[0;32m   1406\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moptions\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1407\u001b[1;33m         run_metadata)\n\u001b[0m\u001b[0;32m   1408\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mUnknownError\u001b[0m: Failed to rename: keras_model.data-00000-of-00001.tempstate16543772219696919801 to: keras_model.data-00000-of-00001 : Access is denied.\r\n; Input/output error\n\t [[{{node save_1/SaveV2}}]]",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mUnknownError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-113-1259041555ac>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[0msaver\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mSaver\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[0msess\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mkeras\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbackend\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_session\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 5\u001b[1;33m \u001b[0msaver\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msave\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msess\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'keras_model'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      6\u001b[0m \u001b[0mpickle\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdump\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtokenizer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtokenizer_file_name\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"wb\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mprotocol\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mpickle\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mHIGHEST_PROTOCOL\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[0mfsx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msave\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mclassifier_file_name\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\tensorflow\\python\\training\\saver.py\u001b[0m in \u001b[0;36msave\u001b[1;34m(self, sess, save_path, global_step, latest_filename, meta_graph_suffix, write_meta_graph, write_state, strip_default_attrs)\u001b[0m\n\u001b[0;32m   1169\u001b[0m           model_checkpoint_path = sess.run(\n\u001b[0;32m   1170\u001b[0m               \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msaver_def\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msave_tensor_name\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1171\u001b[1;33m               {self.saver_def.filename_tensor_name: checkpoint_file})\n\u001b[0m\u001b[0;32m   1172\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1173\u001b[0m         \u001b[0mmodel_checkpoint_path\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mas_str\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel_checkpoint_path\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36mrun\u001b[1;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m    927\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    928\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[1;32m--> 929\u001b[1;33m                          run_metadata_ptr)\n\u001b[0m\u001b[0;32m    930\u001b[0m       \u001b[1;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    931\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_run\u001b[1;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m   1150\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m \u001b[1;32mor\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mhandle\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0mfeed_dict_tensor\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1151\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[1;32m-> 1152\u001b[1;33m                              feed_dict_tensor, options, run_metadata)\n\u001b[0m\u001b[0;32m   1153\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1154\u001b[0m       \u001b[0mresults\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_do_run\u001b[1;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m   1326\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1327\u001b[0m       return self._do_call(_run_fn, feeds, fetches, targets, options,\n\u001b[1;32m-> 1328\u001b[1;33m                            run_metadata)\n\u001b[0m\u001b[0;32m   1329\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1330\u001b[0m       \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_do_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0m_prun_fn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeeds\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfetches\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_do_call\u001b[1;34m(self, fn, *args)\u001b[0m\n\u001b[0;32m   1346\u001b[0m           \u001b[1;32mpass\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1347\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0merror_interpolation\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0minterpolate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmessage\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_graph\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1348\u001b[1;33m       \u001b[1;32mraise\u001b[0m \u001b[0mtype\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnode_def\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mop\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmessage\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1349\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1350\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0m_extend_graph\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mUnknownError\u001b[0m: Failed to rename: keras_model.data-00000-of-00001.tempstate16543772219696919801 to: keras_model.data-00000-of-00001 : Access is denied.\r\n; Input/output error\n\t [[node save_1/SaveV2 (defined at <ipython-input-113-1259041555ac>:3) ]]\n\nCaused by op 'save_1/SaveV2', defined at:\n  File \"C:\\Users\\vaingato.CORPDOM\\AppData\\Local\\Continuum\\anaconda3\\lib\\runpy.py\", line 193, in _run_module_as_main\n    \"__main__\", mod_spec)\n  File \"C:\\Users\\vaingato.CORPDOM\\AppData\\Local\\Continuum\\anaconda3\\lib\\runpy.py\", line 85, in _run_code\n    exec(code, run_globals)\n  File \"C:\\Users\\vaingato.CORPDOM\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\ipykernel_launcher.py\", line 16, in <module>\n    app.launch_new_instance()\n  File \"C:\\Users\\vaingato.CORPDOM\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\traitlets\\config\\application.py\", line 658, in launch_instance\n    app.start()\n  File \"C:\\Users\\vaingato.CORPDOM\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\ipykernel\\kernelapp.py\", line 505, in start\n    self.io_loop.start()\n  File \"C:\\Users\\vaingato.CORPDOM\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\tornado\\platform\\asyncio.py\", line 148, in start\n    self.asyncio_loop.run_forever()\n  File \"C:\\Users\\vaingato.CORPDOM\\AppData\\Local\\Continuum\\anaconda3\\lib\\asyncio\\base_events.py\", line 539, in run_forever\n    self._run_once()\n  File \"C:\\Users\\vaingato.CORPDOM\\AppData\\Local\\Continuum\\anaconda3\\lib\\asyncio\\base_events.py\", line 1775, in _run_once\n    handle._run()\n  File \"C:\\Users\\vaingato.CORPDOM\\AppData\\Local\\Continuum\\anaconda3\\lib\\asyncio\\events.py\", line 88, in _run\n    self._context.run(self._callback, *self._args)\n  File \"C:\\Users\\vaingato.CORPDOM\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\tornado\\ioloop.py\", line 690, in <lambda>\n    lambda f: self._run_callback(functools.partial(callback, future))\n  File \"C:\\Users\\vaingato.CORPDOM\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\tornado\\ioloop.py\", line 743, in _run_callback\n    ret = callback()\n  File \"C:\\Users\\vaingato.CORPDOM\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\tornado\\gen.py\", line 781, in inner\n    self.run()\n  File \"C:\\Users\\vaingato.CORPDOM\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\tornado\\gen.py\", line 742, in run\n    yielded = self.gen.send(value)\n  File \"C:\\Users\\vaingato.CORPDOM\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 357, in process_one\n    yield gen.maybe_future(dispatch(*args))\n  File \"C:\\Users\\vaingato.CORPDOM\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\tornado\\gen.py\", line 209, in wrapper\n    yielded = next(result)\n  File \"C:\\Users\\vaingato.CORPDOM\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 267, in dispatch_shell\n    yield gen.maybe_future(handler(stream, idents, msg))\n  File \"C:\\Users\\vaingato.CORPDOM\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\tornado\\gen.py\", line 209, in wrapper\n    yielded = next(result)\n  File \"C:\\Users\\vaingato.CORPDOM\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 534, in execute_request\n    user_expressions, allow_stdin,\n  File \"C:\\Users\\vaingato.CORPDOM\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\tornado\\gen.py\", line 209, in wrapper\n    yielded = next(result)\n  File \"C:\\Users\\vaingato.CORPDOM\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\ipykernel\\ipkernel.py\", line 294, in do_execute\n    res = shell.run_cell(code, store_history=store_history, silent=silent)\n  File \"C:\\Users\\vaingato.CORPDOM\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\ipykernel\\zmqshell.py\", line 536, in run_cell\n    return super(ZMQInteractiveShell, self).run_cell(*args, **kwargs)\n  File \"C:\\Users\\vaingato.CORPDOM\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 2848, in run_cell\n    raw_cell, store_history, silent, shell_futures)\n  File \"C:\\Users\\vaingato.CORPDOM\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 2874, in _run_cell\n    return runner(coro)\n  File \"C:\\Users\\vaingato.CORPDOM\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\IPython\\core\\async_helpers.py\", line 67, in _pseudo_sync_runner\n    coro.send(None)\n  File \"C:\\Users\\vaingato.CORPDOM\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3049, in run_cell_async\n    interactivity=interactivity, compiler=compiler, result=result)\n  File \"C:\\Users\\vaingato.CORPDOM\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3214, in run_ast_nodes\n    if (yield from self.run_code(code, result)):\n  File \"C:\\Users\\vaingato.CORPDOM\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3296, in run_code\n    exec(code_obj, self.user_global_ns, self.user_ns)\n  File \"<ipython-input-113-1259041555ac>\", line 3, in <module>\n    saver = tf.train.Saver()\n  File \"C:\\Users\\vaingato.CORPDOM\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\tensorflow\\python\\training\\saver.py\", line 832, in __init__\n    self.build()\n  File \"C:\\Users\\vaingato.CORPDOM\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\tensorflow\\python\\training\\saver.py\", line 844, in build\n    self._build(self._filename, build_save=True, build_restore=True)\n  File \"C:\\Users\\vaingato.CORPDOM\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\tensorflow\\python\\training\\saver.py\", line 881, in _build\n    build_save=build_save, build_restore=build_restore)\n  File \"C:\\Users\\vaingato.CORPDOM\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\tensorflow\\python\\training\\saver.py\", line 510, in _build_internal\n    save_tensor = self._AddSaveOps(filename_tensor, saveables)\n  File \"C:\\Users\\vaingato.CORPDOM\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\tensorflow\\python\\training\\saver.py\", line 210, in _AddSaveOps\n    save = self.save_op(filename_tensor, saveables)\n  File \"C:\\Users\\vaingato.CORPDOM\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\tensorflow\\python\\training\\saver.py\", line 124, in save_op\n    tensors)\n  File \"C:\\Users\\vaingato.CORPDOM\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\tensorflow\\python\\ops\\gen_io_ops.py\", line 1920, in save_v2\n    name=name)\n  File \"C:\\Users\\vaingato.CORPDOM\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\op_def_library.py\", line 788, in _apply_op_helper\n    op_def=op_def)\n  File \"C:\\Users\\vaingato.CORPDOM\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\tensorflow\\python\\util\\deprecation.py\", line 507, in new_func\n    return func(*args, **kwargs)\n  File \"C:\\Users\\vaingato.CORPDOM\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\ops.py\", line 3300, in create_op\n    op_def=op_def)\n  File \"C:\\Users\\vaingato.CORPDOM\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\ops.py\", line 1801, in __init__\n    self._traceback = tf_stack.extract_stack()\n\nUnknownError (see above for traceback): Failed to rename: keras_model.data-00000-of-00001.tempstate16543772219696919801 to: keras_model.data-00000-of-00001 : Access is denied.\r\n; Input/output error\n\t [[node save_1/SaveV2 (defined at <ipython-input-113-1259041555ac>:3) ]]\n"
     ]
    }
   ],
   "source": [
    "from keras import backend\n",
    "import tensorflow as tf\n",
    "saver = tf.train.Saver()\n",
    "sess = keras.backend.get_session()\n",
    "saver.save(sess, 'keras_model')\n",
    "pickle.dump(tokenizer, open(tokenizer_file_name, \"wb\"), protocol=pickle.HIGHEST_PROTOCOL)\n",
    "fsx.model.save(classifier_file_name)\n",
    "\n",
    "model = keras.models.load_model(classifier_file_name)\n",
    "\n",
    "saver = tf.train.Saver()\n",
    "sess = keras.backend.get_session()\n",
    "saver.restore(sess, 'keras_model')\n",
    "\n",
    "model.predict(inputs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "not enough values to unpack (expected 2, got 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-108-ab22cebe8b28>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     13\u001b[0m \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     14\u001b[0m     \u001b[0mdocs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcreate_docs\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcolumn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtrain_mode\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtokenizer\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mloaded_tokenizer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mreferance_col\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'name'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 15\u001b[1;33m     \u001b[0mprob_val_matrix\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_class\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mloaded_fsx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdocs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     16\u001b[0m     \u001b[0mpredicted_team_id\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m(\u001b[0m\u001b[1;33m-\u001b[0m\u001b[0mprob_val_matrix\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0margsort\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     17\u001b[0m     \u001b[0mpredicted_team_id\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m(\u001b[0m\u001b[1;33m-\u001b[0m\u001b[0mprob_val_matrix\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0margsort\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: not enough values to unpack (expected 2, got 1)"
     ]
    }
   ],
   "source": [
    "string_to_predict = 'pipeline module does not work properly'\n",
    "column = pd.DataFrame({'name' : [string_to_predict]})\n",
    "\n",
    "\n",
    "\n",
    "train = False\n",
    "if train:\n",
    "    docs = create_docs(data=column, train_mode=False, tokenizer=tokenizer, referance_col='name')\n",
    "    prob_val_matrix, y_class = fsx.predict(docs)\n",
    "    predicted_team_id = le.inverse_transform(y_class)[0]\n",
    "    team_probability = prob_val_matrix[0][y_class]\n",
    "else:\n",
    "    docs = create_docs(data=column, train_mode=False, tokenizer=loaded_tokenizer, referance_col='name')\n",
    "    prob_val_matrix, y_class = loaded_fsx.predict(docs)\n",
    "    predicted_team_id = (-prob_val_matrix).argsort()[0][0]\n",
    "    predicted_team_id = (-prob_val_matrix).argsort()[0][1]\n",
    "\n",
    "# predicted_team_name = team_id_to_name_map[predicted_team_id]\n",
    "# team_probability = prob_val_matrix[0][predicted_team_id]\n",
    "\n",
    "# team_id_b = le.inverse_transform([max_teams[0]])[0]\n",
    "\n",
    "print (str(team_probability) + \" probability it's: \" + predicted_team_name)\n",
    "print (str(prob_val_y[max_teams[1]]) + \" probability it's: \" + predicted_team_name)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # extract NLP features\n",
    "# from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "# from sklearn.pipeline import Pipeline, FeatureUnion\n",
    "# from sklearn.decomposition import PCA as sklearnPCA, TruncatedSVD\n",
    "# from sklearn.base import BaseEstimator, TransformerMixin\n",
    "# class ItemSelector(BaseEstimator, TransformerMixin):\n",
    "\n",
    "#     def __init__(self, key):\n",
    "#         self.key = key\n",
    "\n",
    "#     def fit(self, x, y=None):\n",
    "#         return self\n",
    "\n",
    "#     def transform(self, data_dict):\n",
    "#         return data_dict[self.key]\n",
    "    \n",
    "# tf_idf_3_grams = Pipeline([\n",
    "#                 ('sel', ItemSelector(key='name')),\n",
    "#                 ('tf', TfidfVectorizer(max_features=1000,\n",
    "#                                        strip_accents='unicode', token_pattern=r'\\w{1,}',\n",
    "#                                        ngram_range=(1, 3), use_idf=1, smooth_idf=1, sublinear_tf=1,\n",
    "#                                        stop_words='english')),\n",
    "#                 ('svd', TruncatedSVD(n_components=50))\n",
    "#     ])    \n",
    "# name_features = tf_idf_3_grams.fit_transform(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# train test split: train, valid, test sets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## regularization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "how to avoid overfitting\n",
    "- reduce layers\n",
    "- randomly neglect nodes from producing output in nn (dropout)\n",
    "- add regularization (penelize for large weights: loss + x) L2 regularization\n",
    "\n",
    "how to avoid underfitting - \n",
    "- encrease layers / nodes.\n",
    "- add additional features.\n",
    "\n",
    "to add regularization to layer:\n",
    "model.add(Dense(5, input_shape=(26,), kernal.regulizer=regulizers.l2(0.01), activation='relu'))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# create the model with layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "# Dense is a type of layer (basic type), fully connected layer\n",
    "# first arg is number of neurons in layer\n",
    "# the activation function getting the wieghted sum from all input nodes, and output a number between 0 to 1.\n",
    "# the first layer is hidden layer (and the input defined with param input_shape)\n",
    "model.add(Dense(5, input_shape=(50,), activation='relu'))\n",
    "model.add(Dense(5, activation='relu'))\n",
    "model.add(Dense(1, activation='softmax'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_11 (Dense)             (None, 5)                 255       \n",
      "_________________________________________________________________\n",
      "dense_12 (Dense)             (None, 5)                 30        \n",
      "_________________________________________________________________\n",
      "dense_13 (Dense)             (None, 1)                 6         \n",
      "=================================================================\n",
      "Total params: 291\n",
      "Trainable params: 291\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# compile the module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "# using optimizer (SGC, Adma..) , minimize the loss funtion.\n",
    "# Adam is a variation of SGC, also choose loss func + metrics (printed out)\n",
    "model.compile(Adam(lr=.0001), loss='binary_crossentropy', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# train the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'train_samples' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-59-42f209c78dd5>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m# expect to get numpy array\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;31m# verbose: how much output we want to see\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrain_samples\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtrain_labels\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalidation_split\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0.20\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m10\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m20\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mshuffle\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m: name 'train_samples' is not defined"
     ]
    }
   ],
   "source": [
    "# expect to get numpy array\n",
    "# verbose: how much output we want to see \n",
    "model.fit(train_samples, train_labels, validation_split=0.20, batch_size=10, epochs=20, shuffle=True, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
